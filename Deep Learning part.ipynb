{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libaries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import warnings\n",
    "\n",
    "pd.set_option('max_columns', 120)\n",
    "pd.set_option('max_colwidth', 5000)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let begin by exploying our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18013, 2401)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>xVel1</th>\n",
       "      <th>yVel1</th>\n",
       "      <th>xA1</th>\n",
       "      <th>yA1</th>\n",
       "      <th>xS1</th>\n",
       "      <th>yS1</th>\n",
       "      <th>xC1</th>\n",
       "      <th>yC1</th>\n",
       "      <th>nAC1</th>\n",
       "      <th>nS1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>xVel2</th>\n",
       "      <th>yVel2</th>\n",
       "      <th>xA2</th>\n",
       "      <th>yA2</th>\n",
       "      <th>xS2</th>\n",
       "      <th>yS2</th>\n",
       "      <th>xC2</th>\n",
       "      <th>yC2</th>\n",
       "      <th>nAC2</th>\n",
       "      <th>nS2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>xVel3</th>\n",
       "      <th>yVel3</th>\n",
       "      <th>xA3</th>\n",
       "      <th>yA3</th>\n",
       "      <th>xS3</th>\n",
       "      <th>yS3</th>\n",
       "      <th>xC3</th>\n",
       "      <th>yC3</th>\n",
       "      <th>nAC3</th>\n",
       "      <th>nS3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>xVel4</th>\n",
       "      <th>yVel4</th>\n",
       "      <th>xA4</th>\n",
       "      <th>yA4</th>\n",
       "      <th>xS4</th>\n",
       "      <th>yS4</th>\n",
       "      <th>xC4</th>\n",
       "      <th>yC4</th>\n",
       "      <th>nAC4</th>\n",
       "      <th>nS4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>xVel5</th>\n",
       "      <th>yVel5</th>\n",
       "      <th>xA5</th>\n",
       "      <th>yA5</th>\n",
       "      <th>xS5</th>\n",
       "      <th>yS5</th>\n",
       "      <th>xC5</th>\n",
       "      <th>yC5</th>\n",
       "      <th>nAC5</th>\n",
       "      <th>nS5</th>\n",
       "      <th>...</th>\n",
       "      <th>y196</th>\n",
       "      <th>xVel196</th>\n",
       "      <th>yVel196</th>\n",
       "      <th>xA196</th>\n",
       "      <th>yA196</th>\n",
       "      <th>xS196</th>\n",
       "      <th>yS196</th>\n",
       "      <th>xC196</th>\n",
       "      <th>yC196</th>\n",
       "      <th>nAC196</th>\n",
       "      <th>nS196</th>\n",
       "      <th>x197</th>\n",
       "      <th>y197</th>\n",
       "      <th>xVel197</th>\n",
       "      <th>yVel197</th>\n",
       "      <th>xA197</th>\n",
       "      <th>yA197</th>\n",
       "      <th>xS197</th>\n",
       "      <th>yS197</th>\n",
       "      <th>xC197</th>\n",
       "      <th>yC197</th>\n",
       "      <th>nAC197</th>\n",
       "      <th>nS197</th>\n",
       "      <th>x198</th>\n",
       "      <th>y198</th>\n",
       "      <th>xVel198</th>\n",
       "      <th>yVel198</th>\n",
       "      <th>xA198</th>\n",
       "      <th>yA198</th>\n",
       "      <th>xS198</th>\n",
       "      <th>yS198</th>\n",
       "      <th>xC198</th>\n",
       "      <th>yC198</th>\n",
       "      <th>nAC198</th>\n",
       "      <th>nS198</th>\n",
       "      <th>x199</th>\n",
       "      <th>y199</th>\n",
       "      <th>xVel199</th>\n",
       "      <th>yVel199</th>\n",
       "      <th>xA199</th>\n",
       "      <th>yA199</th>\n",
       "      <th>xS199</th>\n",
       "      <th>yS199</th>\n",
       "      <th>xC199</th>\n",
       "      <th>yC199</th>\n",
       "      <th>nAC199</th>\n",
       "      <th>nS199</th>\n",
       "      <th>x200</th>\n",
       "      <th>y200</th>\n",
       "      <th>xVel200</th>\n",
       "      <th>yVel200</th>\n",
       "      <th>xA200</th>\n",
       "      <th>yA200</th>\n",
       "      <th>xS200</th>\n",
       "      <th>yS200</th>\n",
       "      <th>xC200</th>\n",
       "      <th>yC200</th>\n",
       "      <th>nAC200</th>\n",
       "      <th>nS200</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-489.28</td>\n",
       "      <td>-658.11</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>-1009.24</td>\n",
       "      <td>380.07</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.47</td>\n",
       "      <td>112</td>\n",
       "      <td>33</td>\n",
       "      <td>-1017.81</td>\n",
       "      <td>410.42</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>112</td>\n",
       "      <td>37</td>\n",
       "      <td>-486.08</td>\n",
       "      <td>-660.20</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>45</td>\n",
       "      <td>21</td>\n",
       "      <td>499.77</td>\n",
       "      <td>650.58</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>617.53</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>-1037.88</td>\n",
       "      <td>416.39</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>-467.98</td>\n",
       "      <td>-668.21</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.35</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>502.45</td>\n",
       "      <td>658.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-2.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>-1010.41</td>\n",
       "      <td>398.19</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>112</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-540.61</td>\n",
       "      <td>-670.93</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>-1108.55</td>\n",
       "      <td>453.21</td>\n",
       "      <td>2.96</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.59</td>\n",
       "      <td>112</td>\n",
       "      <td>33</td>\n",
       "      <td>-1103.46</td>\n",
       "      <td>474.24</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>112</td>\n",
       "      <td>34</td>\n",
       "      <td>-494.13</td>\n",
       "      <td>-733.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-2.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.40</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>538.94</td>\n",
       "      <td>585.77</td>\n",
       "      <td>4.43</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>572.02</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.81</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>-1180.26</td>\n",
       "      <td>-138.40</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.99</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>-504.50</td>\n",
       "      <td>-716.51</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>492.70</td>\n",
       "      <td>604.09</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>-1076.23</td>\n",
       "      <td>504.68</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>379.83</td>\n",
       "      <td>-521.18</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-150.48</td>\n",
       "      <td>-865.12</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1147.27</td>\n",
       "      <td>-336.29</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-7.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1389.61</td>\n",
       "      <td>-305.15</td>\n",
       "      <td>4.89</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-44.35</td>\n",
       "      <td>-823.24</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>7.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-675.91</td>\n",
       "      <td>8.22</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>824.23</td>\n",
       "      <td>-45.78</td>\n",
       "      <td>1.65</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1313.09</td>\n",
       "      <td>-886.29</td>\n",
       "      <td>8.22</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1378.23</td>\n",
       "      <td>-802.19</td>\n",
       "      <td>6.29</td>\n",
       "      <td>7.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1038.59</td>\n",
       "      <td>91.30</td>\n",
       "      <td>7.96</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>888.21</td>\n",
       "      <td>-146.53</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>624.18</td>\n",
       "      <td>-83.83</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>339.49</td>\n",
       "      <td>685.16</td>\n",
       "      <td>-12.10</td>\n",
       "      <td>-9.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-487.34</td>\n",
       "      <td>271.33</td>\n",
       "      <td>-6.15</td>\n",
       "      <td>-12.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-680.20</td>\n",
       "      <td>-752.29</td>\n",
       "      <td>5.67</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-308.63</td>\n",
       "      <td>-9.56</td>\n",
       "      <td>-8.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1320.20</td>\n",
       "      <td>503.95</td>\n",
       "      <td>7.04</td>\n",
       "      <td>-6.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>413.66</td>\n",
       "      <td>-996.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-12.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1197.72</td>\n",
       "      <td>-590.32</td>\n",
       "      <td>-11.07</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>703.44</td>\n",
       "      <td>187.44</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-452.43</td>\n",
       "      <td>-632.15</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>-998.40</td>\n",
       "      <td>387.08</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-4.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.50</td>\n",
       "      <td>112</td>\n",
       "      <td>40</td>\n",
       "      <td>-1013.45</td>\n",
       "      <td>388.73</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.93</td>\n",
       "      <td>112</td>\n",
       "      <td>49</td>\n",
       "      <td>-425.48</td>\n",
       "      <td>-705.09</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>580.06</td>\n",
       "      <td>551.02</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>571.86</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-3.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>-1038.16</td>\n",
       "      <td>416.16</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>112</td>\n",
       "      <td>34</td>\n",
       "      <td>-434.37</td>\n",
       "      <td>-684.72</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.46</td>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>561.13</td>\n",
       "      <td>590.30</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>-927.32</td>\n",
       "      <td>415.74</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x1      y1  xVel1  yVel1  xA1  yA1   xS1   yS1   xC1   yC1  nAC1  nS1  \\\n",
       "0  -489.28 -658.11   2.51   3.28  0.0  0.0 -0.26  0.35  0.93 -0.37    45   17   \n",
       "1  -540.61 -670.93  -1.02  -4.76  0.0  0.0 -0.06  0.44  0.24 -0.97    45    2   \n",
       "2   379.83 -521.18   1.86   7.31  0.0  0.0  0.00  0.00  0.00  0.00     1    0   \n",
       "3   888.21 -146.53  -2.67 -18.10  0.0  0.0  0.00  0.00  0.00  0.00     3    0   \n",
       "4  -452.43 -632.15   2.66  -2.63  0.0  0.0 -0.12  0.42  0.01 -1.00    45    5   \n",
       "\n",
       "        x2      y2  xVel2  yVel2  xA2  yA2   xS2   yS2   xC2   yC2  nAC2  nS2  \\\n",
       "0 -1009.24  380.07  -2.83  -1.86  0.0  0.0  0.40 -0.19 -0.88  0.47   112   33   \n",
       "1 -1108.55  453.21   2.96  -2.65  0.0  0.0 -0.37 -0.24  0.81  0.59   112   33   \n",
       "2  -150.48 -865.12  -3.40   8.45  0.0  0.0  0.00  0.00  0.00  0.00     4    0   \n",
       "3   624.18  -83.83  -1.79   9.57  0.0  0.0  0.00  0.00  0.00  0.00     4    0   \n",
       "4  -998.40  387.08   1.87  -4.29  0.0  0.0  0.42 -0.12 -0.86  0.50   112   40   \n",
       "\n",
       "        x3      y3  xVel3  yVel3  xA3  yA3   xS3   yS3   xC3   yC3  nAC3  nS3  \\\n",
       "0 -1017.81  410.42  -2.91   2.47  0.0  0.0  0.25  0.36 -0.66 -0.75   112   37   \n",
       "1 -1103.46  474.24   1.14  -2.29  0.0  0.0 -0.15  0.41  0.60 -0.80   112   34   \n",
       "2  1147.27 -336.29   2.47  -7.49  0.0  0.0 -0.48  0.88  0.00  0.00     6    1   \n",
       "3   339.49  685.16 -12.10  -9.18  0.0  0.0  0.00  0.00  0.00  0.00     4    0   \n",
       "4 -1013.45  388.73   2.01   0.17  0.0  0.0  0.09 -0.43 -0.38  0.93   112   49   \n",
       "\n",
       "        x4      y4  xVel4  yVel4  xA4  yA4   xS4   yS4   xC4   yC4  nAC4  nS4  \\\n",
       "0  -486.08 -660.20  -0.50  -3.48  0.0  0.0 -0.15  0.41  0.65 -0.76    45   21   \n",
       "1  -494.13 -733.68   0.66  -2.98  0.0  0.0  0.39 -0.20 -0.92  0.40    45    5   \n",
       "2  1389.61 -305.15   4.89   6.94  0.0  0.0  0.00  0.00  0.00  0.00     2    0   \n",
       "3  -487.34  271.33  -6.15 -12.80  0.0  0.0  0.00  0.00  0.00  0.00     4    0   \n",
       "4  -425.48 -705.09   0.52   2.27  0.0  0.0  0.31 -0.31 -0.68  0.73    45    6   \n",
       "\n",
       "       x5      y5  xVel5  yVel5  xA5  yA5   xS5   yS5   xC5   yC5  nAC5  nS5  \\\n",
       "0  499.77  650.58  -1.79   3.12  0.0  0.0 -0.19  0.40  0.87 -0.49    37   13   \n",
       "1  538.94  585.77   4.43  -1.31  0.0  0.0  0.22 -0.38 -0.63  0.78    37   11   \n",
       "2  -44.35 -823.24  -2.28   7.21  0.0  0.0  0.00  0.00  0.00  0.00     2    0   \n",
       "3 -680.20 -752.29   5.67  -4.07  0.0  0.0  0.00  0.00  0.00  0.00    61    0   \n",
       "4  580.06  551.02  -2.06  -0.57  0.0  0.0  0.22 -0.38 -0.02  1.00    37   12   \n",
       "\n",
       "   ...    y196  xVel196  yVel196  xA196  yA196  xS196  yS196  xC196  yC196  \\\n",
       "0  ...  617.53     1.15    -3.13    0.0    0.0  -0.02  -0.44   0.02   1.00   \n",
       "1  ...  572.02     3.06     0.52    0.0    0.0  -0.37  -0.24   0.59   0.81   \n",
       "2  ... -675.91     8.22     4.28    0.0    0.0   0.00   0.00   0.00   0.00   \n",
       "3  ... -308.63    -9.56    -8.82    0.0    0.0   0.00   0.00   0.00   0.00   \n",
       "4  ...  571.86     0.23    -3.62    0.0    0.0   0.37   0.23   0.89  -0.46   \n",
       "\n",
       "   nAC196  nS196     x197    y197  xVel197  yVel197  xA197  yA197  xS197  \\\n",
       "0      37     17 -1037.88  416.39    -0.96    -1.76    0.0    0.0  -0.19   \n",
       "1      37      3 -1180.26 -138.40     0.51     3.53    0.0    0.0   0.00   \n",
       "2       3      0   824.23  -45.78     1.65     7.97    0.0    0.0   0.00   \n",
       "3       6      0  1320.20  503.95     7.04    -6.85    0.0    0.0   0.00   \n",
       "4      37     12 -1038.16  416.16     1.42     2.26    0.0    0.0  -0.38   \n",
       "\n",
       "   yS197  xC197  yC197  nAC197  nS197     x198    y198  xVel198  yVel198  \\\n",
       "0   0.40   0.35  -0.94     112     29  -467.98 -668.21    -1.82     3.24   \n",
       "1   0.00   0.14   0.99     108      0  -504.50 -716.51    -1.82    -2.85   \n",
       "2   0.00   0.00   0.00       4      0 -1313.09 -886.29     8.22     3.46   \n",
       "3   0.00   0.00   0.00       0      0   413.66 -996.01     0.35   -12.97   \n",
       "4   0.22   0.87  -0.49     112     34  -434.37 -684.72     0.61     2.09   \n",
       "\n",
       "   xA198  yA198  xS198  yS198  xC198  yC198  nAC198  nS198     x199    y199  \\\n",
       "0    0.0    0.0   0.44   0.00  -0.94   0.35      45     22   502.45  658.55   \n",
       "1    0.0    0.0   0.38   0.22  -0.99  -0.12      45     11   492.70  604.09   \n",
       "2    0.0    0.0   0.00   0.00   0.00   0.00       3      0 -1378.23 -802.19   \n",
       "3    0.0    0.0   0.00   0.00   0.00   0.00      40      1  1197.72 -590.32   \n",
       "4    0.0    0.0   0.31   0.32  -0.89   0.46      45     13   561.13  590.30   \n",
       "\n",
       "   xVel199  yVel199  xA199  yA199  xS199  yS199  xC199  yC199  nAC199  nS199  \\\n",
       "0     0.12    -2.93    0.0    0.0  -0.06   0.44   0.61  -0.79      37      5   \n",
       "1    -1.75     1.29    0.0    0.0  -0.42   0.12   1.00   0.09      37      6   \n",
       "2     6.29     7.16    0.0    0.0   0.00   0.00   0.00   0.00       4      0   \n",
       "3   -11.07    -3.72    0.0    0.0   0.00   0.00   0.00   0.00       7      0   \n",
       "4     0.54     3.63    0.0    0.0  -0.39   0.19   0.74  -0.67      37     10   \n",
       "\n",
       "      x200    y200  xVel200  yVel200  xA200  yA200  xS200  yS200  xC200  \\\n",
       "0 -1010.41  398.19     1.60    -4.17    0.0    0.0   0.33   0.29  -0.87   \n",
       "1 -1076.23  504.68    -1.37    -4.43    0.0    0.0  -0.15   0.41  -0.46   \n",
       "2 -1038.59   91.30     7.96     1.33    0.0    0.0   0.00   0.00   0.00   \n",
       "3   703.44  187.44     1.04    -2.19    0.0    0.0   0.00   0.00   0.00   \n",
       "4  -927.32  415.74    -3.35     2.54    0.0    0.0   0.00   0.00  -1.00   \n",
       "\n",
       "   yC200  nAC200  nS200  Class   \n",
       "0  -0.50     112     40       1  \n",
       "1  -0.89     112      9       1  \n",
       "2   0.00       4      0       1  \n",
       "3   0.00       3      1       0  \n",
       "4  -0.10     112      0       1  \n",
       "\n",
       "[5 rows x 2401 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = r'AI_ML_Assessment\\swarm_train_data.csv'\n",
    "train_df = pd.read_csv(dataset_path, low_memory=False)\n",
    "\n",
    "print(train_df.shape)\n",
    "#Let's check the first five row of our dataset\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we have a dataset of shape(18013, 2401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18013\n"
     ]
    }
   ],
   "source": [
    "num_rows = train_df.shape[0]\n",
    "# len(dataframe)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of columns in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401\n"
     ]
    }
   ],
   "source": [
    "num_cols = train_df.shape[1]\n",
    "# len(dataframe.columns)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a column with a datatype of object, we need to convert that to numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select_dtypes(include=['object'])\n",
    "train_df['x1'][12649]\n",
    "train_df['x1'] = pd.to_numeric(train_df.x1, errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "#Adjust the class label\n",
    "\n",
    "train_df = train_df.rename({'Class ': 'Class'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the dataset for training\n",
    "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x1', 'y1', 'xVel1', 'yVel1', 'xA1', 'yA1', 'xS1', 'yS1', 'xC1', 'yC1',\n",
       "       ...\n",
       "       'xVel200', 'yVel200', 'xA200', 'yA200', 'xS200', 'yS200', 'xC200',\n",
       "       'yC200', 'nAC200', 'nS200'],\n",
       "      dtype='object', length=2400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = train_df.columns[train_df.columns!='Class']\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class']\n"
     ]
    }
   ],
   "source": [
    "output_cols = [train_df.columns[-1]]\n",
    "print(output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First convert all columns to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(train_df)\n",
    "# inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then convert data to Tensor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs_array).type(torch.float32)\n",
    "targets = torch.tensor(targets_array).type(torch.float32)\n",
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I simply split the data into train and validation in the ration of 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.2 # between 0.1 and 0.2\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a batch size and add out tensors to a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Logistic Regression Model\n",
    "\n",
    "\n",
    "Here we simply have a feedforward model with just a single layer, It takes in input features of 2400 and output 0 or 1 (Non-flocking or Flocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(input_cols)\n",
    "num_classes = 2\n",
    "input_size,num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwarmModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)               \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear (xb)                       \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.cross_entropy(out, torch.max(targets,1)[1])                           # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out, torch.max(targets,1)[1])\n",
    "        acc = accuracy(out, targets)#     \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'],result['val_acc']))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a model using the SwarmModel class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes nan or infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0129,  0.0015,  0.0050,  ..., -0.0199,  0.0031, -0.0021],\n",
       "         [ 0.0002, -0.0093, -0.0092,  ...,  0.0061,  0.0131,  0.0094]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.0034, -0.0176], requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwarmModel()\n",
    "\n",
    "#Let's check out the weights and biases of the model using model.paramet\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result,epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 131.2275390625, 'val_acc': 5.03933572769165}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 288.4337, val_acc: 4.9091\n",
      "Epoch [40], val_loss: 288.4337, val_acc: 4.9091\n",
      "Epoch [60], val_loss: 288.4337, val_acc: 4.9091\n",
      "Epoch [80], val_loss: 288.4337, val_acc: 4.9091\n",
      "Epoch [100], val_loss: 288.4337, val_acc: 4.9091\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1e-2\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let plot a graph to see how loss grows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [r['val_loss'] for r in history1]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's save our model to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'swarm-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You can load the pretrained model using\n",
    "\n",
    "model2 = model.load_state_dict(torch.load('swarm-logistic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model2, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondae7109cdd099845309956b5ac56e40405"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
